# DB-001: IColumnsDb<T> Column Family Abstraction — Implementation Plan

## Overview

Implement a comptime-generic `ColumnsDb(T)` abstraction that wraps N `Database` instances (one per enum variant of `T`) into a single logical database with column family support. This mirrors Nethermind's `IColumnsDb<TKey>` interface and unblocks proper receipts and blob transaction storage.

**Key design decisions:**
1. Use `std.EnumArray(T, Database)` for zero-allocation dense column mapping (same pattern as `DbProvider`)
2. Each column IS a `Database` — no new vtable needed for individual columns
3. `ColumnsWriteBatch(T)` wraps N `WriteBatch` instances, one per column
4. `ColumnDbSnapshot(T)` wraps N `DbSnapshot` instances, one per column
5. `MemColumnsDb(T)` is the concrete in-memory implementation (wraps N `MemoryDatabase`)
6. Column enum definitions (`ReceiptsColumns`, `BlobTxsColumns`) live in `columns.zig` alongside the generic struct

**File to create:** `client/db/columns.zig`
**File to modify:** `client/db/root.zig` (add re-exports)

## Step-by-Step Implementation Order

### Step 1: `ReceiptsColumns` enum

**File:** `client/db/columns.zig`

```zig
/// Column families for receipt storage.
/// Matches Nethermind's `ReceiptsColumns` enum from `Nethermind.Db/ReceiptsColumns.cs`.
pub const ReceiptsColumns = enum {
    /// Default column — receipt data indexed by receipt hash.
    default,
    /// Transaction index — receipt data indexed by transaction hash.
    transactions,
    /// Block index — receipt data indexed by block number/hash.
    blocks,

    pub fn to_string(self: ReceiptsColumns) []const u8 {
        return switch (self) {
            .default => "Default",
            .transactions => "Transactions",
            .blocks => "Blocks",
        };
    }
};
```

**Tests:**
- Enum has exactly 3 variants
- `to_string()` returns correct Nethermind-compatible strings

---

### Step 2: `BlobTxsColumns` enum

**File:** `client/db/columns.zig` (append)

```zig
/// Column families for blob transaction storage (EIP-4844).
/// Matches Nethermind's `BlobTxsColumns` enum from `Nethermind.Db/BlobTxsColumns.cs`.
pub const BlobTxsColumns = enum {
    /// Full blob transaction data (including 128KB blobs).
    full_blob_txs,
    /// Light blob transaction metadata (without blob payload).
    light_blob_txs,
    /// Transactions that have been included in finalized blocks.
    processed_txs,

    pub fn to_string(self: BlobTxsColumns) []const u8 {
        return switch (self) {
            .full_blob_txs => "FullBlobTxs",
            .light_blob_txs => "LightBlobTxs",
            .processed_txs => "ProcessedTxs",
        };
    }
};
```

**Tests:**
- Enum has exactly 3 variants
- `to_string()` returns correct Nethermind-compatible strings

---

### Step 3: `ColumnsDb(T)` — core generic struct

**File:** `client/db/columns.zig` (append)

The central abstraction. Generic over any enum type `T`. Holds an `EnumArray(T, Database)` mapping each column key to a `Database` handle. Does NOT own the underlying databases — lifetime is caller's responsibility (same as `DbProvider`).

```zig
/// Generic column family database.
///
/// Mirrors Nethermind's `IColumnsDb<TKey>` interface. Each enum variant of `T`
/// maps to a separate `Database` instance. The `ColumnsDb` does not own the
/// underlying databases; lifetime management is the caller's responsibility.
///
/// `T` must be a Zig `enum` type (validated at comptime).
pub fn ColumnsDb(comptime T: type) type {
    comptime {
        if (@typeInfo(T) != .@"enum") {
            @compileError("ColumnsDb requires an enum type, got " ++ @typeName(T));
        }
    }

    return struct {
        const Self = @This();

        columns: std.EnumArray(T, Database),

        /// Get the `Database` for a specific column.
        pub fn getColumnDb(self: *const Self, key: T) Database { ... }

        /// Return all column keys (comptime-known slice of enum fields).
        pub fn columnKeys() []const T { ... }
    };
}
```

**Tests:**
- `getColumnDb()` returns correct `Database` for each column key
- `columnKeys()` returns all enum variants
- Comptime error when `T` is not an enum (verify with `@compileError` check — may need a comptime test or doc comment)

---

### Step 4: `ColumnsWriteBatch(T)` — cross-column atomic writes

**File:** `client/db/columns.zig` (append)

Wraps one `WriteBatch` per column. `getColumnBatch(key)` returns the batch for that column. `commit()` commits all column batches. `deinit()` releases all batch memory.

```zig
/// Cross-column write batch.
///
/// Mirrors Nethermind's `IColumnsWriteBatch<TKey>`. Wraps one `WriteBatch` per
/// column. Operations are accumulated per-column and committed together.
///
/// NOTE: Atomicity is per-column (each column's WriteBatch commits independently).
/// True cross-column atomicity requires the underlying backend to support it
/// (e.g., RocksDB WriteBatch across column families). For `MemoryDatabase`
/// backends, commits are sequential per-column.
pub fn ColumnsWriteBatch(comptime T: type) type {
    return struct {
        const Self = @This();

        batches: std.EnumArray(T, WriteBatch),

        pub fn init(allocator: std.mem.Allocator, columns: std.EnumArray(T, Database)) Self { ... }
        pub fn getColumnBatch(self: *Self, key: T) *WriteBatch { ... }
        pub fn commit(self: *Self) Error!void { ... }
        pub fn deinit(self: *Self) void { ... }
        pub fn pending(self: *const Self) usize { ... }
    };
}
```

**Tests:**
- `getColumnBatch()` returns correct `WriteBatch` per column
- `commit()` applies all pending ops across all columns
- `deinit()` frees all memory (leak check via `std.testing.allocator`)
- Partial failure: if one column commit fails, error is returned (and previously committed columns are not rolled back — document this)

---

### Step 5: `ColumnDbSnapshot(T)` — cross-column consistent reads

**File:** `client/db/columns.zig` (append)

Wraps one `DbSnapshot` per column. `getColumnSnapshot(key)` returns the snapshot for that column. `deinit()` releases all snapshots.

```zig
/// Cross-column snapshot for consistent point-in-time reads.
///
/// Mirrors Nethermind's `IColumnDbSnapshot<TKey>`. Each column gets its own
/// `DbSnapshot`, all created at the same logical point in time.
pub fn ColumnDbSnapshot(comptime T: type) type {
    return struct {
        const Self = @This();

        snapshots: std.EnumArray(T, DbSnapshot),

        pub fn getColumnSnapshot(self: *const Self, key: T) DbSnapshot { ... }
        pub fn deinit(self: *Self) void { ... }
    };
}
```

**Tests:**
- `getColumnSnapshot()` returns correct snapshot per column
- Snapshot isolates later writes (write after snapshot, read from snapshot sees old data)
- `deinit()` releases all snapshot resources

---

### Step 6: Add `startWriteBatch()` and `createSnapshot()` to `ColumnsDb(T)`

**File:** `client/db/columns.zig` (modify `ColumnsDb`)

Add methods to `ColumnsDb(T)`:

```zig
/// Create a cross-column write batch.
pub fn startWriteBatch(self: *const Self, allocator: std.mem.Allocator) ColumnsWriteBatch(T) { ... }

/// Create a cross-column snapshot.
pub fn createSnapshot(self: *const Self) Error!ColumnDbSnapshot(T) { ... }
```

**Tests:**
- `startWriteBatch()` returns a batch targeting all columns
- `createSnapshot()` returns snapshots from all columns
- End-to-end: write via batch → commit → read back from each column

---

### Step 7: `MemColumnsDb(T)` — in-memory concrete implementation

**File:** `client/db/columns.zig` (append)

Owns N `MemoryDatabase` instances, one per column. Provides `columnsDb()` to get the type-erased `ColumnsDb(T)` interface.

```zig
/// In-memory column family database.
///
/// Mirrors Nethermind's `MemColumnsDb<TKey>`. Each column is backed by a
/// separate `MemoryDatabase`. Owns all underlying databases and releases
/// them on `deinit()`.
pub fn MemColumnsDb(comptime T: type) type {
    return struct {
        const Self = @This();

        databases: std.EnumArray(T, MemoryDatabase),
        allocator: std.mem.Allocator,

        pub fn init(allocator: std.mem.Allocator) Self { ... }
        pub fn deinit(self: *Self) void { ... }
        pub fn columnsDb(self: *Self) ColumnsDb(T) { ... }
    };
}
```

**Tests:**
- `init()` creates N `MemoryDatabase` instances
- `deinit()` frees all memory (leak check)
- `columnsDb().getColumnDb(key)` returns working `Database` per column
- Full round-trip: put via column db → get via column db
- Column isolation: write to column A, verify column B is unaffected
- WriteBatch round-trip via `columnsDb().startWriteBatch()`
- Snapshot isolation via `columnsDb().createSnapshot()`

---

### Step 8: Update `root.zig` — re-export new public types

**File:** `client/db/root.zig` (modify)

Add:
```zig
const columns = @import("columns.zig");

pub const ColumnsDb = columns.ColumnsDb;
pub const ColumnsWriteBatch = columns.ColumnsWriteBatch;
pub const ColumnDbSnapshot = columns.ColumnDbSnapshot;
pub const MemColumnsDb = columns.MemColumnsDb;
pub const ReceiptsColumns = columns.ReceiptsColumns;
pub const BlobTxsColumns = columns.BlobTxsColumns;
```

**Tests:**
- Existing `refAllDecls` test in `root.zig` will verify compilation of new module

---

## Files Summary

### Files to Create
| File | Contents |
|------|----------|
| `client/db/columns.zig` | `ReceiptsColumns`, `BlobTxsColumns`, `ColumnsDb(T)`, `ColumnsWriteBatch(T)`, `ColumnDbSnapshot(T)`, `MemColumnsDb(T)` + all tests |

### Files to Modify
| File | Change |
|------|--------|
| `client/db/root.zig` | Add `columns` import + 6 new re-exports |

## Tests Summary

| # | Test | What It Verifies |
|---|------|-----------------|
| 1 | `ReceiptsColumns` has 3 variants | Enum completeness |
| 2 | `ReceiptsColumns.to_string()` matches Nethermind | String compatibility |
| 3 | `BlobTxsColumns` has 3 variants | Enum completeness |
| 4 | `BlobTxsColumns.to_string()` matches Nethermind | String compatibility |
| 5 | `ColumnsDb.getColumnDb()` returns correct Database | Column access |
| 6 | `ColumnsDb.columnKeys()` returns all variants | Enum iteration |
| 7 | `ColumnsWriteBatch.getColumnBatch()` returns correct batch | Batch per-column access |
| 8 | `ColumnsWriteBatch.commit()` applies all ops | Cross-column commit |
| 9 | `ColumnsWriteBatch.deinit()` frees all memory | Leak check |
| 10 | `ColumnDbSnapshot.getColumnSnapshot()` returns correct snapshot | Snapshot per-column access |
| 11 | `ColumnDbSnapshot` isolates later writes | Snapshot consistency |
| 12 | `ColumnDbSnapshot.deinit()` releases resources | Cleanup |
| 13 | `ColumnsDb.startWriteBatch()` creates working batch | Integration |
| 14 | `ColumnsDb.createSnapshot()` creates working snapshot | Integration |
| 15 | `MemColumnsDb.init()` creates N databases | Construction |
| 16 | `MemColumnsDb.deinit()` frees all memory | Leak check |
| 17 | `MemColumnsDb` column isolation | Write A doesn't affect B |
| 18 | `MemColumnsDb` full round-trip (put/get per column) | End-to-end correctness |
| 19 | `MemColumnsDb` write batch round-trip | Batch integration |
| 20 | `MemColumnsDb` snapshot isolation | Snapshot integration |

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| **Cross-column atomicity**: `ColumnsWriteBatch.commit()` commits each column independently — partial failure leaves some columns committed | Data inconsistency on crash | Document limitation clearly. True cross-column atomicity will come with RocksDB backend (single WriteBatch across column families). For in-memory use this is acceptable. |
| **`MemColumnsDb` DbName assignment**: Each `MemoryDatabase` needs a `DbName`, but column families don't map 1:1 to `DbName` variants | Wrong name reported via `Database.name()` | Use a convention: `.receipts` for all ReceiptsColumns databases, `.blob_transactions` for all BlobTxsColumns databases. Alternatively, add a `DbName` parameter to `MemColumnsDb.init()`. |
| **Snapshot failure propagation**: If creating a snapshot for one column succeeds but another fails, need to clean up already-created snapshots | Resource leak | Use `errdefer` to deinit already-created snapshots on partial failure in `createSnapshot()`. |
| **EnumArray over large enums**: If someone passes an enum with many variants, the arrays could be large | Stack overflow for very large enums | Not a concern for our 3-variant enums. Document that `T` should be a small enum. |
| **Memory ownership confusion**: `ColumnsDb(T)` does NOT own databases; `MemColumnsDb(T)` DOES | Use-after-free if caller frees early | Document ownership model clearly in doc comments. Follow same pattern as `DbProvider` (non-owning) vs `MemoryDatabase` (owning). |

## Verification Against Acceptance Criteria

1. **`ColumnsDb` struct generic over enum type** — Step 3: `ColumnsDb(T)` comptime generic
2. **`GetColumnDb(key)`** — Step 3: `getColumnDb(self, key) Database`
3. **`ColumnKeys()`** — Step 3: `columnKeys() []const T` (comptime)
4. **`StartWriteBatch()`** — Steps 4 + 6: `ColumnsWriteBatch(T)` + `startWriteBatch()`
5. **`CreateSnapshot()`** — Steps 5 + 6: `ColumnDbSnapshot(T)` + `createSnapshot()`
6. **`ReceiptsColumns` enum** — Step 1: `{ default, transactions, blocks }`
7. **`BlobTxsColumns` enum** — Step 2: `{ full_blob_txs, light_blob_txs, processed_txs }`
8. **In-memory implementation** — Step 7: `MemColumnsDb(T)`
9. **Tests for all public functions** — 20 tests covering all public APIs
10. **Follows existing patterns** — Uses `EnumArray`, vtable `Database`, `WriteBatch`, `DbSnapshot`, arena allocation

## Commit Strategy

Each step should be a separate atomic commit:
1. `♻️ refactor(db): add ReceiptsColumns and BlobTxsColumns enums`
2. `♻️ refactor(db): add ColumnsDb(T) generic struct`
3. `♻️ refactor(db): add ColumnsWriteBatch(T) cross-column batch`
4. `♻️ refactor(db): add ColumnDbSnapshot(T) cross-column snapshot`
5. `♻️ refactor(db): add startWriteBatch and createSnapshot to ColumnsDb`
6. `♻️ refactor(db): add MemColumnsDb(T) in-memory implementation`
7. `♻️ refactor(db): export column family types from root.zig`

Or, if the entire implementation is small enough (~300-400 LOC), a single commit:
`♻️ refactor(db): add ColumnsDb(T) column family abstraction (DB-001)`
