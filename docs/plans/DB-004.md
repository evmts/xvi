# DB-004: Add merge operation to Database vtable

## Overview

Add an optional `merge` function pointer to the `Database.VTable`, following the exact same pattern as the existing `write_batch` optional field. This mirrors Nethermind's `IMergeableKeyValueStore.Merge(key, value, flags)` method.

The change is purely additive — all existing backends continue to work with `merge: null` (the default). The merge operation enables efficient read-modify-write patterns (e.g., RocksDB native merge operators) without requiring a read-before-write round-trip.

## Nethermind Reference

- `IMergeableKeyValueStore` (`Nethermind.Core/IKeyValueStore.cs:115-118`): defines `Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)`
- `IWriteBatch` (`Nethermind.Core/IWriteBatch.cs:8`): extends `IMergeableKeyValueStore` — write batches must also support merge
- `DbOnTheRocks` (`Nethermind.Db.Rocks/DbOnTheRocks.cs:954-986`): concrete merge implementation using RocksDB native merge operators
- `ColumnDb` (`Nethermind.Db.Rocks/ColumnDb.cs:63-64`): delegates merge to main DB
- `InMemoryWriteBatch` (`Nethermind.Db/InMemoryWriteBatch.cs:43-46`): throws `NotSupportedException` for merge

## Step-by-step Implementation Order

### Step 1: Add `merge` variant to `WriteBatchOp`

**File:** `client/db/adapter.zig` (line ~682)

```zig
pub const WriteBatchOp = union(enum) {
    put: struct { key: []const u8, value: []const u8 },
    del: struct { key: []const u8 },
    merge: struct { key: []const u8, value: []const u8 },
};
```

**Rationale:** Add the union variant first so it's available when we wire up the VTable and WriteBatch. This is a data type change only — no behavior change yet. Existing `switch` statements on `WriteBatchOp` will need to handle the new variant (compiler will catch them).

**Tests:** None yet — tested indirectly in later steps.

### Step 2: Add `merge` optional function pointer to `VTable`

**File:** `client/db/adapter.zig` (line ~429, after `write_batch`)

```zig
/// Apply a merge operation (RocksDB native merge operator).
///
/// Backends that support native merge operators (e.g., RocksDB) should
/// implement this for efficient read-modify-write without read-before-write.
/// If `null`, merge operations will return `error.UnsupportedOperation`.
///
/// Mirrors Nethermind's `IMergeableKeyValueStore.Merge(key, value, flags)`.
merge: ?*const fn (ptr: *anyopaque, key: []const u8, value: []const u8, flags: WriteFlags) Error!void = null,
```

**Rationale:** Optional field defaulting to `null`, identical pattern to `write_batch`. All existing vtable constructions (MockDb, TrackingDb, FailingDb, AtomicDb, etc.) continue to compile because the default is `null`.

**Tests:** None yet — tested in Step 3.

### Step 3: Add convenience methods to `Database`

**File:** `client/db/adapter.zig` (after `start_write_batch`, around line ~514)

```zig
/// Returns true if the backend supports the `merge` operation.
pub fn supports_merge(self: Database) bool {
    return self.vtable.merge != null;
}

/// Apply a merge operation with default write flags.
///
/// Returns `error.UnsupportedOperation` if the backend does not support merge.
/// Mirrors Nethermind's `IMergeableKeyValueStore.Merge(key, value)`.
pub fn merge(self: Database, key: []const u8, value: []const u8) Error!void {
    return self.merge_with_flags(key, value, WriteFlags.none);
}

/// Apply a merge operation with explicit write flags.
///
/// Returns `error.UnsupportedOperation` if the backend does not support merge.
/// Mirrors Nethermind's `IMergeableKeyValueStore.Merge(key, value, flags)`.
pub fn merge_with_flags(self: Database, key: []const u8, value: []const u8, flags: WriteFlags) Error!void {
    const merge_fn = self.vtable.merge orelse return error.UnsupportedOperation;
    return merge_fn(self.ptr, key, value, flags);
}
```

**Design decision:** Return `error.UnsupportedOperation` (not `error.StorageError`) when merge is not supported. This matches the error semantics used elsewhere (e.g., `DbSnapshot.iterator` returns `error.UnsupportedOperation` when not implemented). This is more precise than `StorageError` and mirrors Nethermind's `NotSupportedException`.

**Tests to add:**
1. `"Database supports_merge reports false when absent"` — create MockDb (no merge in vtable), verify `supports_merge() == false`
2. `"Database supports_merge reports true when present"` — create a mock with merge fn ptr set, verify `supports_merge() == true`
3. `"Database merge returns UnsupportedOperation when absent"` — call `merge()` on MockDb, expect `error.UnsupportedOperation`
4. `"Database merge delegates to vtable"` — create mock that tracks calls, verify key/value/flags are forwarded
5. `"Database merge_with_flags forwards flags"` — verify non-default flags are passed through

### Step 4: Update `WriteBatch.commit` fallback path to handle merge ops

**File:** `client/db/adapter.zig` (line ~786, in `WriteBatch.commit`)

Update the sequential fallback `switch` to handle the new `.merge` variant:

```zig
for (self.ops.items) |op| {
    switch (op) {
        .put => |p| try self.target.put(p.key, p.value),
        .del => |d| try self.target.delete(d.key),
        .merge => |m| try self.target.merge_with_flags(m.key, m.value, WriteFlags.none),
    }
}
```

**Rationale:** The fallback path applies ops one-by-one when the backend doesn't have a native `write_batch`. Merge ops in the fallback path call `merge_with_flags` on the target database. If the target doesn't support merge either, this will propagate `error.UnsupportedOperation`.

**Tests:** Covered in Step 5.

### Step 5: Add `WriteBatch.merge()` method

**File:** `client/db/adapter.zig` (after `WriteBatch.delete`, around line ~762)

```zig
/// Queue a merge operation. Both key and value are copied into the batch arena.
///
/// The merge will be applied when `commit()` is called. If the target database
/// does not support merge, `commit()` will return `error.UnsupportedOperation`
/// when it reaches this operation.
pub fn merge(self: *WriteBatch, key: []const u8, value: []const u8) Error!void {
    const alloc = self.arena.allocator();
    const owned_key = alloc.dupe(u8, key) catch return error.OutOfMemory;
    const owned_val = alloc.dupe(u8, value) catch return error.OutOfMemory;
    self.ops.append(self.ops_allocator, .{ .merge = .{ .key = owned_key, .value = owned_val } }) catch return error.OutOfMemory;
}
```

**Tests to add:**
1. `"WriteBatch merge accumulates operations"` — add merge ops, verify `pending()` count
2. `"WriteBatch commit applies merge ops via fallback path"` — use a TrackingDb-style mock that implements merge, verify ops applied
3. `"WriteBatch commit with merge ops via atomic path"` — use AtomicDb-style mock, verify merge ops appear in the ops slice
4. `"WriteBatch merge propagates OutOfMemory"` — use failing allocator, expect `error.OutOfMemory`

### Step 6: Update `Database.init` comptime helper to accept optional `merge`

**File:** `client/db/adapter.zig` (line ~541, in `Database.init`)

Add `merge` to the comptime struct parameter and generate the wrapper:

```zig
pub fn init(comptime T: type, ptr: *T, comptime fns: struct {
    // ... existing fields ...
    write_batch: ?*const fn (self: *T, ops: []const WriteBatchOp) Error!void = null,
    merge: ?*const fn (self: *T, key: []const u8, value: []const u8, flags: WriteFlags) Error!void = null,
}) Database {
    const Wrapper = struct {
        // ... existing wrappers ...

        fn merge_impl(raw: *anyopaque, key: []const u8, value: []const u8, flags: WriteFlags) Error!void {
            const typed: *T = @ptrCast(@alignCast(raw));
            const merge_fn = fns.merge orelse unreachable;
            return merge_fn(typed, key, value, flags);
        }

        const vtable = VTable{
            // ... existing fields ...
            .write_batch = if (fns.write_batch == null) null else write_batch_impl,
            .merge = if (fns.merge == null) null else merge_impl,
        };
    };
    // ...
}
```

**Rationale:** Follows the exact same pattern as `write_batch`. The `merge` parameter defaults to `null`, so all existing callers (MemoryDatabase, NullDb, ReadOnlyDb, etc.) compile without changes.

**Tests to add:**
1. `"Database.init with merge"` — create a backend with merge, verify `supports_merge() == true` and dispatch works
2. `"Database.init without merge defaults to null"` — verify `supports_merge() == false` when merge is omitted

### Step 7: Verify existing backends compile unchanged

**Files to verify (no changes needed):**
- `client/db/memory.zig` — no merge support (null default)
- `client/db/null.zig` — no merge support (null default)
- `client/db/read_only.zig` — no merge support (null default)
- `client/db/columns.zig` — uses `WriteBatch` which already handles merge via commit

**Verification:** Run `zig build test` — all existing tests must pass without modification.

## Files to Create

None.

## Files to Modify

| File | Changes |
|------|---------|
| `client/db/adapter.zig` | Add `merge` variant to `WriteBatchOp`, add `merge` to `VTable`, add `supports_merge()`/`merge()`/`merge_with_flags()` methods, add `WriteBatch.merge()`, update `WriteBatch.commit` fallback, update `Database.init` comptime helper, add tests |

## Tests to Write

All tests go in `client/db/adapter.zig` (inline test blocks, following existing pattern):

| Test Name | What it Verifies |
|-----------|------------------|
| `"Database supports_merge reports false when absent"` | MockDb (no merge) → `supports_merge() == false` |
| `"Database supports_merge reports true when present"` | Mock with merge → `supports_merge() == true` |
| `"Database merge returns UnsupportedOperation when absent"` | MockDb → `merge()` returns `error.UnsupportedOperation` |
| `"Database merge delegates to vtable"` | Tracking mock → key/value forwarded correctly |
| `"Database merge_with_flags forwards flags"` | Tracking mock → non-default flags passed through |
| `"WriteBatch merge accumulates operations"` | `pending()` reflects merge ops |
| `"WriteBatch commit applies merge ops via fallback path"` | Merge-capable TrackingDb sees merge ops |
| `"WriteBatch commit with merge ops via atomic path"` | AtomicDb receives merge ops in batch |
| `"WriteBatch merge propagates OutOfMemory"` | Failing allocator → `error.OutOfMemory` |
| `"WriteBatchOp merge variant holds key and value"` | Construct and destructure `.merge` variant |
| `"Database.init with merge generates correct dispatch"` | Comptime init with merge → dispatch works |
| `"Database.init without merge defaults to null"` | Comptime init without merge → `supports_merge() == false` |

## Risks and Mitigations

| Risk | Likelihood | Mitigation |
|------|-----------|------------|
| Name collision: `WriteFlags.merge` method vs `WriteBatchOp.merge` variant | Low | They're in different types. `WriteFlags.merge` is a bitwise-OR method on the flags struct. `WriteBatchOp.merge` is a union variant. No ambiguity in Zig's type system. |
| `ReadFlags.merge` method name collision with `Database.merge` | Low | `ReadFlags.merge` is a method on `ReadFlags` struct, `Database.merge` is on `Database`. Different types, no collision. |
| Existing switch statements on `WriteBatchOp` don't handle `.merge` | Medium | Zig compiler enforces exhaustive switches — any missing case is a compile error. Step 4 handles the only switch in the codebase. |
| Future backends may not expect merge ops in write batches | Low | The atomic `write_batch` path passes the full ops slice including merge variants. Backend implementations must handle all `WriteBatchOp` variants or the compiler will enforce it. |
| In-memory backends can't meaningfully implement merge | N/A | This is expected — they leave `merge: null` in the vtable. Callers check `supports_merge()` before using. Matches Nethermind where `InMemoryWriteBatch` throws `NotSupportedException`. |

## Verification Against Acceptance Criteria

1. **VTable has optional merge function pointer** — Step 2 adds it, defaulting to `null`
2. **Follows write_batch pattern** — Same optional field pattern, same comptime init pattern, same convenience method pattern
3. **WriteFlags support** — `merge_with_flags()` accepts `WriteFlags`, matching Nethermind's `Merge(key, value, flags)`
4. **WriteBatch supports merge** — Steps 4-5 add `WriteBatch.merge()` and handle in both atomic and fallback commit paths
5. **Existing backends unchanged** — Step 7 verifies all backends compile without modification
6. **Comprehensive tests** — 12 test cases covering all new code paths
7. **No Voltaire types needed** — Confirmed: DB layer is below Voltaire's state management

## Build & Test Commands

```bash
# After each step:
zig fmt client/db/adapter.zig
zig build
zig build test

# Verify all DB tests pass:
zig build test 2>&1 | grep -E "(PASS|FAIL|error)"
```
